{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804846c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Pré-processamento de Dados\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Certifique-se de que o arquivo de texto existe\n",
    "file_path = 'texto.txt'\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Erro: O arquivo '{file_path}' não foi encontrado.\")\n",
    "    print(\"Certifique-se de que o arquivo está no mesmo diretório do script.\")\n",
    "    exit()\n",
    "\n",
    "# Carrega o texto do arquivo\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Cria o vocabulário (mapeamento de caracteres para inteiros)\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "n_vocab = len(chars)\n",
    "n_chars = len(text)\n",
    "\n",
    "# Cria as sequências de treinamento\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    seq_out = text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Modelo RNN (LSTM)\n",
    "# --------------------------------------------------\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # Correção principal: Pega a saída apenas do último passo de tempo\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Preparação para o Treino\n",
    "# --------------------------------------------------\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataX, dataY):\n",
    "        self.dataX = torch.tensor(dataX, dtype=torch.long)\n",
    "        self.dataY = torch.tensor(dataY, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataX)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataX[idx], self.dataY[idx]\n",
    "\n",
    "dataset = TextDataset(dataX, dataY)\n",
    "batch_size = 128 \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instancia o modelo, a função de perda e o otimizador\n",
    "model = CharRNN(n_vocab, hidden_size=256, output_size=n_vocab, n_layers=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Define o número de épocas para o treinamento\n",
    "num_epochs = 5\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Treinamento do Modelo\n",
    "# --------------------------------------------------\n",
    "\n",
    "print(\"Iniciando o treinamento do modelo...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # O hidden state é inicializado DENTRO do loop de batches\n",
    "    for inputs, labels in dataloader:\n",
    "        \n",
    "        # AQUI ESTÁ A CORREÇÃO PRINCIPAL:\n",
    "        # A nova batch_size é obtida do tamanho do lote atual de inputs.\n",
    "        batch_size = inputs.size(0)\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Desconecta os estados ocultos\n",
    "        hidden = tuple([h.data for h in hidden])\n",
    "        \n",
    "        output, hidden = model(inputs, hidden)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Época [{epoch+1}/{num_epochs}], Perda: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Treinamento concluído.\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Geração de Texto\n",
    "# --------------------------------------------------\n",
    "\n",
    "def generate_text(model, start_string, length):\n",
    "    # Converte a string de início para um tensor\n",
    "    input_eval = torch.tensor([char_to_int[s] for s in start_string]).unsqueeze(0)\n",
    "    \n",
    "    text_generated = start_string\n",
    "    model.eval() # Coloca o modelo em modo de avaliação\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(1)\n",
    "        for _ in range(length):\n",
    "            output, hidden = model(input_eval, hidden)\n",
    "            \n",
    "            # Obtém o caractere previsto\n",
    "            predicted_char = int_to_char[torch.argmax(output[-1]).item()]\n",
    "            \n",
    "            # Adiciona ao texto gerado\n",
    "            text_generated += predicted_char\n",
    "            \n",
    "            # Atualiza a entrada para a próxima etapa\n",
    "            input_eval = torch.tensor([[char_to_int[predicted_char]]])\n",
    "    \n",
    "    return text_generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb28da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Texto Gerado ---\n",
      "Format                      : Matroska Format  : Version 4 / Version                                                                                                 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemplo de uso:\n",
    "print(\"\\n--- Texto Gerado ---\")\n",
    "# Use uma string de início que faça sentido com o estilo do autor\n",
    "start_text = \"Format                      : Matroska Format  : Version 4 / Vers\" \n",
    "generated_text = generate_text(model, start_string=start_text, length=100)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".FGV_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
